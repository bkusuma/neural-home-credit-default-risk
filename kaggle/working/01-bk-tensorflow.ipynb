{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95733ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For TensorFlow version 2.12 or earlier\n",
    "\n",
    "# Command Line Tools\n",
    "# sudo rm -rf /Library/Developer/CommandLineTools\n",
    "# xcode-select --install\n",
    "\n",
    "# Create conda environment\n",
    "# conda create -n tensorenv python=3.10\n",
    "# conda activate tensorenv\n",
    "\n",
    "# And then Tensorflow dependencies\n",
    "# conda install apple::tensorflow-deps\n",
    "# conda install notebook -y\n",
    "\n",
    "# pip install pandas matplotlib scikit-learn scipy plotly --upgrade\n",
    "# pip install tensorflow-macos==2.12 numpy==1.23.5 tensorflow-metal==0.8.0 --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d59a985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Instantiate an optimizer.\n",
    "# optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "# # Instantiate a loss function.\n",
    "# loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# # Prepare the training dataset.\n",
    "# batch_size = 32\n",
    "\n",
    "\n",
    "# # Get a fresh model\n",
    "# model = get_model()\n",
    "\n",
    "# # Instantiate an optimizer to train the model.\n",
    "# optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "# # Instantiate a loss function.\n",
    "# loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# # Prepare the metrics.\n",
    "# train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "# val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6dee4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 3\n",
    "# for epoch in range(epochs):\n",
    "#     print(f\"\\nStart of epoch {epoch}\")\n",
    "\n",
    "#     # Iterate over the batches of the dataset.\n",
    "#     for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "#         # Open a GradientTape to record the operations run\n",
    "#         # during the forward pass, which enables auto-differentiation.\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             # Run the forward pass of the layer.\n",
    "#             # The operations that the layer applies\n",
    "#             # to its inputs are going to be recorded\n",
    "#             # on the GradientTape.\n",
    "#             logits = model(x_batch_train, training=True)  # Logits for this minibatch\n",
    "\n",
    "#             # Compute the loss value for this minibatch.\n",
    "#             loss_value = loss_fn(y_batch_train, logits)\n",
    "\n",
    "#         # Use the gradient tape to automatically retrieve\n",
    "#         # the gradients of the trainable variables with respect to the loss.\n",
    "#         grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "\n",
    "#         # Run one step of gradient descent by updating\n",
    "#         # the value of the variables to minimize the loss.\n",
    "#         optimizer.apply(grads, model.trainable_weights)\n",
    "\n",
    "#         # Log every 100 batches.\n",
    "#         if step % 100 == 0:\n",
    "#             print(\n",
    "#                 f\"Training loss (for 1 batch) at step {step}: {float(loss_value):.4f}\"\n",
    "#             )\n",
    "#             print(f\"Seen so far: {(step + 1) * batch_size} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28f8583",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e3379e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kagglehub\n",
    "\n",
    "# # Download the latest version.\n",
    "# kagglehub.competition_download('home-credit-default-risk')\n",
    "\n",
    "# # # Download a single file.\n",
    "# # kagglehub.competition_download('home-credit-default-risk', path='train.csv')\n",
    "\n",
    "# # # Download a competition or file, even if previously downloaded to cache. \n",
    "# # kagglehub.competition_download('home-credit-default-risk', force_download=True)\n",
    "\n",
    "# # '/Users/bharat/.cache/kagglehub/competitions/home-credit-default-risk'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a5a4206",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_path = \"../src/\"\n",
    "lib_path = \"../lib/\"\n",
    "input_path = \"../../kaggle/input/home-credit-default-risk/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83396a81",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFoundError",
     "evalue": "dlopen(/opt/homebrew/Caskroom/miniforge/base/envs/kerasenv/lib/python3.10/site-packages/tensorflow-plugins/libmetal_plugin.dylib, 0x0006): Symbol not found: __ZN10tensorflow32_OpDef_AttrDef_default_instance_E\n  Referenced from: <C6A8F8FF-CAC2-366C-AE46-E6AD29B2AF1E> /opt/homebrew/Caskroom/miniforge/base/envs/kerasenv/lib/python3.10/site-packages/tensorflow-plugins/libmetal_plugin.dylib\n  Expected in:     <1CF9FA7D-55BD-35DC-9D66-42816309C430> /opt/homebrew/Caskroom/miniforge/base/envs/kerasenv/lib/python3.10/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, Dropout, Input\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/kerasenv/lib/python3.10/site-packages/tensorflow/__init__.py:457\u001b[0m\n\u001b[1;32m    455\u001b[0m _plugin_dir \u001b[38;5;241m=\u001b[39m _os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(_s, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow-plugins\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(_plugin_dir):\n\u001b[0;32m--> 457\u001b[0m   \u001b[43m_ll\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_library\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_plugin_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    458\u001b[0m   \u001b[38;5;66;03m# Load Pluggable Device Library\u001b[39;00m\n\u001b[1;32m    459\u001b[0m   _ll\u001b[38;5;241m.\u001b[39mload_pluggable_device_library(_plugin_dir)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/kerasenv/lib/python3.10/site-packages/tensorflow/python/framework/load_library.py:151\u001b[0m, in \u001b[0;36mload_library\u001b[0;34m(library_location)\u001b[0m\n\u001b[1;32m    148\u001b[0m     kernel_libraries \u001b[38;5;241m=\u001b[39m [library_location]\n\u001b[1;32m    150\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m lib \u001b[38;5;129;01min\u001b[39;00m kernel_libraries:\n\u001b[0;32m--> 151\u001b[0m     \u001b[43mpy_tf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_LoadLibrary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlib\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    155\u001b[0m       errno\u001b[38;5;241m.\u001b[39mENOENT,\n\u001b[1;32m    156\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe file or folder to load kernel libraries from does not exist.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    157\u001b[0m       library_location)\n",
      "\u001b[0;31mNotFoundError\u001b[0m: dlopen(/opt/homebrew/Caskroom/miniforge/base/envs/kerasenv/lib/python3.10/site-packages/tensorflow-plugins/libmetal_plugin.dylib, 0x0006): Symbol not found: __ZN10tensorflow32_OpDef_AttrDef_default_instance_E\n  Referenced from: <C6A8F8FF-CAC2-366C-AE46-E6AD29B2AF1E> /opt/homebrew/Caskroom/miniforge/base/envs/kerasenv/lib/python3.10/site-packages/tensorflow-plugins/libmetal_plugin.dylib\n  Expected in:     <1CF9FA7D-55BD-35DC-9D66-42816309C430> /opt/homebrew/Caskroom/miniforge/base/envs/kerasenv/lib/python3.10/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import sys\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.models import Sequential\n",
    "from sklearn import set_config\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow import keras\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "tf.random.set_seed(42)\n",
    "set_config(transform_output=\"pandas\")\n",
    "\n",
    "sys.path.insert(0, src_path)\n",
    "data_processing = joblib.load(lib_path + \"data_processing.joblib\")\n",
    "preprocessor = data_processing[\"preprocessor\"]\n",
    "non_co_cols = data_processing[\"non_co_cols\"]\n",
    "merged_application_test = data_processing[\"merged_application_train\"]\n",
    "from threshold_tuner import ClassificationThresholdTuner\n",
    "del sys.path[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a303c10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    for metric in history.history.keys():\n",
    "        if not metric.startswith(\"val\"):\n",
    "            plt.plot(history.history[metric], label=metric)\n",
    "            if f\"val_{metric}\" in history.history.keys():\n",
    "                plt.plot(history.history[f\"val_{metric}\"], label=f\"val_{metric}\")\n",
    "            plt.title(metric)\n",
    "            plt.legend()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b85f626e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macOS-15.4.1-arm64-arm-64bit\n",
      "Python 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:51:49) [Clang 16.0.6 ]\n",
      "NumPy 1.26.4\n",
      "Pandas 2.2.3\n",
      "Scikit-Learn 1.6.1\n",
      "SciPy 1.15.2\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSciPy \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscipy\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorFlow \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mtf\u001b[49m\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeras \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkeras\u001b[38;5;241m.\u001b[39m__version__\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m gpu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tf\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlist_physical_devices(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m'\u001b[39m))\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"{platform.platform()}\")\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"NumPy {np.__version__}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sklearn.__version__}\")\n",
    "print(f\"SciPy {scipy.__version__}\")\n",
    "print()\n",
    "print(f\"TensorFlow {tf.__version__}\")\n",
    "print(f\"Keras {keras.__version__}\")\n",
    "\n",
    "gpu = len(tf.config.list_physical_devices('GPU'))>0\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")\n",
    "print(\"Number of GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print(tf.reduce_sum(tf.random.normal([1000, 1000])))\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5653af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import operation in python script for training\n",
    "# import tensorflow as tf\n",
    "# zero_out_module = tf.load_op_library('./zero_out.so')\n",
    "# print(zero_out_module.zero_out([[1, 2], [3, 4]]).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539d3cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate data\n",
    "X = merged_application_test.drop(columns=[\"TARGET\", \"SK_ID_CURR\"])\n",
    "y = merged_application_test[\"TARGET\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df383c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data 80/10/10 for training, validation, and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, train_size=0.5, test_size=0.5, random_state=42)\n",
    "# transform data\n",
    "X_train = preprocessor.fit_transform(X_train)\n",
    "X_val = preprocessor.transform(X_val)\n",
    "X_test = preprocessor.transform(X_test)\n",
    "# drop columns with collinear relationships (Pearson's correlation coefficients > 0.8)\n",
    "X_train = X_train.drop(columns=non_co_cols)\n",
    "X_val = X_val.drop(columns=non_co_cols)\n",
    "X_test = X_test.drop(columns=non_co_cols)\n",
    "# scale remaining data\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfd6592",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# create y arrays for multiclass modeling\n",
    "y_array_train = to_categorical(y_train)\n",
    "y_array_val = to_categorical(y_val)\n",
    "y_array_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4e25d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "# model.compile(optimizer=\"adam\", loss=loss_fn, metrics=[\"accuracy\"])\n",
    "from keras.metrics import AUC\n",
    "# from keras.optimizers.legacy import Adam\n",
    "\n",
    "\n",
    "# Binary Classification Scenario 1\n",
    "# --------------------------------\n",
    "# If your model outputs a softmax vector (probabilities for each class),\n",
    "# your target variable should be one-hot encoded.\n",
    "# You can use tf.keras.utils.to_categorical to convert integer labels to one-hot vectors.\n",
    "# array([[0., 1.],\n",
    "#        [1., 0.],\n",
    "#        [1., 0.],\n",
    "#        ...,\n",
    "#        [1., 0.],\n",
    "#        [0., 1.],\n",
    "#        [1., 0.]], dtype=float32)\n",
    "# For multi-class classification with one-hot encoded targets (softmax activation), use categorical_crossentropy.\n",
    "def create_model_1():\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(X_train.shape[1],)))  # Input layer\n",
    "    model.add(Dense(4, activation=\"relu\"))  # Hidden layer 1\n",
    "    model.add(Dense(2, activation=\"softmax\"))  # Output layer\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", AUC(name=\"auc\")],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model_1 = create_model_1()\n",
    "\n",
    "history_1 = model_1.fit(\n",
    "    X_train,\n",
    "    y_array_train,\n",
    "    validation_data=(X_val, y_array_val),\n",
    "    epochs=16,\n",
    "    batch_size=64,\n",
    ")\n",
    "\n",
    "# history = model.fit(\n",
    "#     x_train,\n",
    "#     y_train,\n",
    "#     batch_size=64,\n",
    "#     epochs=2,\n",
    "#     # We pass some validation for\n",
    "#     # monitoring validation loss and metrics\n",
    "#     # at the end of each epoch\n",
    "#     validation_data=(x_val, y_val),\n",
    "# )\n",
    "\n",
    "# model.compile(\n",
    "#     optimizer=keras.optimizers.RMSprop(learning_rate=1e-3),\n",
    "#     loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "#     metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a36567",
   "metadata": {},
   "source": [
    "Many built-in optimizers, losses, and metrics are available\n",
    "In general, you won't have to create your own losses, metrics, or optimizers from scratch, because what you need is likely to be already part of the Keras API:\n",
    "\n",
    "Optimizers:\n",
    "\n",
    "- SGD() (with or without momentum)\n",
    "- RMSprop()\n",
    "- Adam()\n",
    "- etc.  \n",
    "\n",
    "\n",
    "Losses:\n",
    "\n",
    "- MeanSquaredError()\n",
    "- KLDivergence()\n",
    "- CosineSimilarity()\n",
    "- etc.\n",
    "\n",
    "\n",
    "Metrics:\n",
    "\n",
    "- AUC()\n",
    "- Precision()\n",
    "- Recall()\n",
    "- etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db84dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdaa22ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "y_pred = model_1.predict(X_test)\n",
    "# y_array_pred = np.argmax(y_array_pred, axis=1)\n",
    "# y_array_test = pd.DataFrame(y_array_test)\n",
    "\n",
    "y_pred_flat = np.argmax(y_pred, axis=1)\n",
    "\n",
    "\n",
    "print(classification_report(y_true=y_test, y_pred=y_pred_flat, zero_division=np.nan))\n",
    "ConfusionMatrixDisplay.from_predictions(y_true=y_test, y_pred=y_pred_flat);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ee6912",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.evaluate(X_test, y_array_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ff35ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a3cd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard --logdir=/full_path_to_your_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02919c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras.callbacks.TensorBoard(\n",
    "#     log_dir=\"/full_path_to_your_logs\",\n",
    "#     histogram_freq=0,  # How often to log histogram visualizations\n",
    "#     embeddings_freq=0,  # How often to log embedding visualizations\n",
    "#     update_freq=\"epoch\",\n",
    "# )  # How often to write logs (default: once per epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b987e9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binary Classification Scenario 2\n",
    "# --------------------------------\n",
    "# If you're doing binary classification, ensure your final layer\n",
    "# has one neuron and uses a sigmoid activation function.\n",
    "# 0         1\n",
    "# 1         0\n",
    "# 2         0\n",
    "# 3         0\n",
    "# 4         0\n",
    "#          ..\n",
    "# 307506    0\n",
    "# 307507    0\n",
    "# 307508    0\n",
    "# 307509    1\n",
    "# 307510    0\n",
    "# Name: TARGET, Length: 307511, dtype: int64\n",
    "# For binary classification with a single output neuron (sigmoid activation), use binary_crossentropy.\n",
    "def create_model_2():\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(X_train.shape[1],)))  # Input layer\n",
    "    model.add(Dense(64, activation=\"relu\"))  # Hidden layer 1\n",
    "    model.add(Dense(32, activation=\"relu\"))  # Hidden layer 2\n",
    "    model.add(Dense(16, activation=\"relu\"))  # Hidden layer 3\n",
    "    model.add(Dense(16, activation=\"relu\"))  # Hidden layer 4\n",
    "    model.add(Dense(1, activation=\"sigmoid\"))  # Output layer\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\"accuracy\", AUC(name=\"auc\")],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model_2 = create_model_2()\n",
    "history_2 = model_2.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eeaab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3084be56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.evaluate(X_test, y_test, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99588b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_2.predict(X_test)\n",
    "# y_array_pred = np.argmax(y_array_pred, axis=1)\n",
    "# y_array_test = pd.DataFrame(y_array_test)\n",
    "\n",
    "y_pred_flat = np.argmax(y_pred, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237e93a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = pd.DataFrame(\n",
    "#     {\"zero\": 0, \"pred\": np.nan}, index=range(len(preds))\n",
    "# )\n",
    "\n",
    "# for i in range(len(preds)):\n",
    "#     y_pred.iloc[i, 1] = preds[i][0]\n",
    "\n",
    "# y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458e4f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # y_pred = (y_pred>0.25) # It will evaluate the logical expression y_pred>0.25 and return True or False \n",
    "# model_2_pred = pd.DataFrame(columns=[\"Pred\"], index=y_test.index)\n",
    "# model_2_pred['Pred'] = np.where(y_pred > 0.50, 1, 0)\n",
    "\n",
    "# model_2_pred.tail(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1720bee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_2 = ClassificationThresholdTuner()\n",
    "\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "best_threshold_2 = tuner_2.tune_threshold(\n",
    "    y_true=y_test, \n",
    "    target_classes=[0,1],\n",
    "    y_pred_proba=y_pred_flat,\n",
    "    metric=f1_score,\n",
    "    average='macro',\n",
    "    higher_is_better=True,\n",
    "    max_iterations=5\n",
    ")\n",
    "\n",
    "tuner_2.print_stats_proba(\n",
    "    y_true=y_test, \n",
    "    target_classes=[0,1], \n",
    "    y_pred_proba=y_pred_flat) \n",
    "\n",
    "\n",
    "tuned_pred_2 = tuner_2.get_predictions(\n",
    "    target_classes=[0, 1],\n",
    "    y_pred_proba=y_pred_flat,\n",
    "    default_class=0,\n",
    "    thresholds=best_threshold_2,\n",
    ")\n",
    "tuned_pred_2 = pd.Series(tuned_pred_2)\n",
    "tuned_pred_2 = tuned_pred_2.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f124c665",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_flat = np.argmax(y_test, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51429461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(classification_report(y_true=y_test, y_pred=tuned_pred_2))\n",
    "print(classification_report(y_test, y_pred_flat, zero_division=np.nan))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8fff23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ConfusionMatrixDisplay.from_predictions(y_test, tuned_pred_2)\n",
    "ConfusionMatrixDisplay.from_estimator(model_2, X_test, y_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d22a1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from keras.metrics import AUC\n",
    "\n",
    "\n",
    "\n",
    "# Binary Classification Scenario 3\n",
    "# --------------------------------\n",
    "# If your model outputs a single value (e.g., a probability between 0 and 1 for binary classification),\n",
    "# your target variable should be a single value (0 or 1).\n",
    "# If you're doing multi-class classification, ensure your final layer has a number of neurons equal to the number of classes\n",
    "# and uses a softmax activation function. Also ensure that your labels are properly one-hot encoded.\n",
    "# For multi-class classification with integer targets, use sparse_categorical_crossentropy.\n",
    "def create_model_3():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Input(shape=(X_train.shape[1],)))  # Input layer\n",
    "\n",
    "    model.add(Dense(32, activation=\"relu\"))  # Hidden layer 1\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(16, activation=\"relu\"))  # Hidden layer 2\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(8, activation=\"relu\"))  # Hidden layer 3\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(y_train.nunique(), activation=\"softmax\"))  # Output layer\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\", AUC(name=\"auc\")]\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model_3 = create_model_3()\n",
    "history_3 = model_3.fit(\n",
    "    X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=256, callbacks=[EarlyStopping(patience=3, restore_best_weights=True,\n",
    "    monitor=\"val_auc\", mode=\"min\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0879f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f702003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred_proba = model_3.predict(X_test)\n",
    "\n",
    "# Calculate AUC\n",
    "test_auc = roc_auc_score(y_test, y_pred_proba[:, 1])  # Assuming binary classification and probabilities for class 1\n",
    "\n",
    "print(f\"Validation AUC: {test_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82bb44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_3 = ClassificationThresholdTuner()\n",
    "\n",
    "tuner_3.print_stats_table(\n",
    "    y_true=y_test,\n",
    "    target_classes=[0, 1],\n",
    "    y_pred_proba=y_pred_proba,\n",
    "    num_ranges=len(y_test),\n",
    ")\n",
    "\n",
    "tuner_3.print_stats_proba(\n",
    "    y_true=y_test, target_classes=[0, 1], y_pred_proba=y_pred_proba\n",
    ")\n",
    "\n",
    "tuner_3.plot_by_threshold(\n",
    "    y_true=y_test, target_classes=[0, 1], y_pred_proba=y_pred_proba\n",
    ")\n",
    "\n",
    "best_threshold_3 = tuner_3.tune_threshold(\n",
    "    y_true=y_test,\n",
    "    target_classes=[0, 1],\n",
    "    y_pred_proba=y_pred_proba,\n",
    "    metric=f1_score,\n",
    "    average=\"macro\",\n",
    "    higher_is_better=True,\n",
    "    max_iterations=5,\n",
    ")\n",
    "\n",
    "tuned_pred_3 = tuner_3.get_predictions(\n",
    "    target_classes=[0, 1],\n",
    "    y_pred_proba=y_pred_proba,\n",
    "    default_class=0,\n",
    "    thresholds=best_threshold_3,\n",
    ")\n",
    "tuned_pred_3 = pd.Series(tuned_pred_3)\n",
    "tuned_pred_3 = tuned_pred_3.astype(int)\n",
    "\n",
    "print(\n",
    "    classification_report(\n",
    "        y_test,\n",
    "        tuned_pred_3,\n",
    "        zero_division=np.nan,\n",
    "    )\n",
    ")\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, tuned_pred_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d963764",
   "metadata": {},
   "source": [
    "## Import test data and preliminary merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6029fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_file = \"application_test.csv\"\n",
    "application_test = pd.read_csv(input_path + test_data_file)\n",
    "\n",
    "bureau_loans_and_balances = pd.read_csv(lib_path + \"bureau_loans_and_balances.csv\")\n",
    "\n",
    "installments_payments = pd.read_csv(input_path + \"installments_payments.csv\")\n",
    "POS_CASH_balance = pd.read_csv(input_path + \"POS_CASH_balance.csv\")\n",
    "credit_card_balance = pd.read_csv(input_path + \"credit_card_balance.csv\")\n",
    "\n",
    "application_test = pd.merge(application_test, bureau_loans_and_balances, how=\"left\", on=\"SK_ID_CURR\")\n",
    "\n",
    "idx = pd.DataFrame(application_test['SK_ID_CURR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6407c8",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe28c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTALL steps\n",
    "installments_payments.drop(columns=\"SK_ID_PREV\", inplace=True)\n",
    "installments_payments.columns = [col + \"_INSTALL\" for col in installments_payments.columns]\n",
    "merge_INSTALL = pd.merge(idx, installments_payments, how=\"inner\", left_on=\"SK_ID_CURR\", right_on=\"SK_ID_CURR_INSTALL\")\n",
    "merge_INSTALL.drop(columns=\"SK_ID_CURR_INSTALL\", inplace=True)\n",
    "merge_INSTALL = merge_INSTALL.sort_values(by=\"NUM_INSTALMENT_NUMBER_INSTALL\", ascending=False).drop_duplicates(subset=[\"SK_ID_CURR\"])\n",
    "\n",
    "# POS steps\n",
    "POS_CASH_balance.drop(columns=\"SK_ID_PREV\", inplace=True)\n",
    "POS_CASH_balance.columns = [col + \"_POS\" for col in POS_CASH_balance.columns]\n",
    "merge_POS = pd.merge(idx, POS_CASH_balance, how=\"inner\", left_on=\"SK_ID_CURR\", right_on=\"SK_ID_CURR_POS\")\n",
    "merge_POS.drop(columns=\"SK_ID_CURR_POS\", inplace=True)\n",
    "merge_POS = merge_POS.sort_values(by=\"MONTHS_BALANCE_POS\", ascending=False).drop_duplicates(subset=[\"SK_ID_CURR\"])\n",
    "\n",
    "# CC step\n",
    "credit_card_balance.drop(columns=\"SK_ID_PREV\", inplace=True)\n",
    "credit_card_balance.columns = [col + \"_CC\" for col in credit_card_balance.columns]\n",
    "merge_CC = pd.merge(idx, credit_card_balance, how=\"inner\", left_on=\"SK_ID_CURR\", right_on=\"SK_ID_CURR_CC\")\n",
    "merge_CC.drop(columns=\"SK_ID_CURR_CC\", inplace=True)\n",
    "merge_CC = merge_CC.sort_values(by=\"MONTHS_BALANCE_CC\", ascending=False).drop_duplicates(subset=[\"SK_ID_CURR\"])\n",
    "\n",
    "# Merge down filtered data\n",
    "balances_and_payments = pd.merge(merge_INSTALL, merge_POS, how=\"outer\", left_on=\"SK_ID_CURR\", right_on=\"SK_ID_CURR\")\n",
    "balances_and_payments = pd.merge(balances_and_payments, merge_CC, how=\"outer\", left_on=\"SK_ID_CURR\", right_on=\"SK_ID_CURR\")\n",
    "\n",
    "# Drop object columns\n",
    "# balances_and_payments.select_dtypes(include=\"object\")\n",
    "balances_and_payments.drop(labels=[\"NAME_CONTRACT_STATUS_POS\", \"NAME_CONTRACT_STATUS_CC\"], axis=1, inplace=True)\n",
    "\n",
    "# Merge down onto test file\n",
    "application_test = pd.merge(application_test, balances_and_payments, how=\"left\", on=\"SK_ID_CURR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164a1d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pop off index ids\n",
    "ids = application_test.pop(\"SK_ID_CURR\")\n",
    "\n",
    "# transform data\n",
    "application_test = preprocessor.transform(application_test)\n",
    "\n",
    "# drop columns with collinear relationships (Pearson's correlation coefficients > 0.8)\n",
    "application_test.drop(columns=non_co_cols, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d10af4",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566bc0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "preds = hgb_3.predict_proba(application_test)[:,1]\n",
    "\n",
    "output = pd.DataFrame({\"SK_ID_CURR\": ids,\n",
    "                       \"TARGET\": preds})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba38cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc97da8",
   "metadata": {},
   "source": [
    "## Export submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5ac0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission_file = \"sample_submission.csv\"\n",
    "sample_submission_df = pd.read_csv(input_path + sample_submission_file)\n",
    "sample_submission_df[\"TARGET\"] = preds\n",
    "submission_path = \"submissions/\"\n",
    "sample_submission_df.to_csv(submission_path + \"02_hgb_3_kaggle.csv\", index=False)\n",
    "sample_submission_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a4862d",
   "metadata": {},
   "source": [
    "# Save model for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0124f250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # saving model to lib folder for future use\n",
    "# to_save = {\"model\" : calibrated_clf}\n",
    "# joblib_filename = \"modelling.joblib\"\n",
    "\n",
    "# joblib.dump(to_save, src_path + joblib_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cbd0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_scce():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Input(shape=(X_train.shape[1],)))  # Input layer\n",
    "\n",
    "    model.add(Dense(32, activation=\"relu\"))  # Hidden layer 1\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(16, activation=\"relu\"))  # Hidden layer 2\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(8, activation=\"relu\"))  # Hidden layer 3\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(y_train.nunique(), activation=\"softmax\"))  # Output layer\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"adam\",\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\", AUC(name=\"auc\")],\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model_scce = create_model_scce()\n",
    "history_scce = model_scce.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=25,\n",
    "    batch_size=54,\n",
    "    callbacks=[\n",
    "        EarlyStopping(\n",
    "            patience=3, restore_best_weights=True, monitor=\"val_auc\", mode=\"min\"\n",
    "        )\n",
    "    ],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kerasenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
